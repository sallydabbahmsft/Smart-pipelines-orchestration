{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import time, json\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "def utc_now():\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "try: runId = runId\n",
        "except NameError: runId = f\"manual_{int(time.time())}\"\n",
        "\n",
        "try: workloadName = workloadName\n",
        "except NameError: workloadName = \"unknown\"\n",
        "\n",
        "try: tier = tier\n",
        "except NameError: tier = \"unknown\"\n",
        "\n",
        "try: weight = weight\n",
        "except NameError: weight = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import time, json\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "start_ts = time.time()\n",
        "start_utc = utc_now()\n",
        "\n",
        "# Medium dataset (~8M). Tune up/down by changing n.\n",
        "n = 8_000_000\n",
        "\n",
        "# Fact-like table\n",
        "fact = (spark.range(0, n)\n",
        "            .withColumn(\"customer_id\", (F.col(\"id\") % 500_000).cast(\"int\"))\n",
        "            .withColumn(\"amount\", (F.rand(seed=11) * 1000).cast(\"double\"))\n",
        "            .withColumn(\"day\", (F.col(\"id\") % 30).cast(\"int\"))\n",
        "            .repartition(120, \"customer_id\"))  # moderate shuffle pressure\n",
        "\n",
        "# Dimension-like table (smaller)\n",
        "dim = (spark.range(0, 500_000)\n",
        "           .withColumnRenamed(\"id\", \"customer_id\")\n",
        "           .withColumn(\"segment\", (F.col(\"customer_id\") % 20).cast(\"int\")))\n",
        "\n",
        "# Join + window aggregation (typical “reporting”)\n",
        "joined = fact.join(F.broadcast(dim), on=\"customer_id\", how=\"inner\")\n",
        "\n",
        "w = Window.partitionBy(\"segment\").orderBy(F.desc(\"amount\"))\n",
        "top = (joined.withColumn(\"rn\", F.row_number().over(w))\n",
        "             .where(F.col(\"rn\") <= 50)\n",
        "             .groupBy(\"segment\")\n",
        "             .agg(F.count(\"*\").alias(\"top_count\"),\n",
        "                  F.avg(\"amount\").alias(\"avg_amount\")))\n",
        "\n",
        "rows = top.count()  # force execution\n",
        "\n",
        "duration = round(time.time() - start_ts, 2)\n",
        "result = {\n",
        "    \"runId\": runId, \"workloadName\": workloadName, \"tier\": tier, \"weight\": weight,\n",
        "    \"startTimeUtc\": start_utc, \"durationSec\": duration,\n",
        "    \"metric\": {\"resultRows\": rows, \"n\": n}\n",
        "}\n",
        "print(result)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "mssparkutils.notebook.exit(json.dumps(result))"
      ]
    }
  ]
}