{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import time, json\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "def utc_now():\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "try: runId = runId\n",
        "except NameError: runId = f\"manual_{int(time.time())}\"\n",
        "\n",
        "try: workloadName = workloadName\n",
        "except NameError: workloadName = \"unknown\"\n",
        "\n",
        "try: tier = tier\n",
        "except NameError: tier = \"unknown\"\n",
        "\n",
        "try: weight = weight\n",
        "except NameError: weight = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import time, json\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "start_ts = time.time()\n",
        "start_utc = utc_now()\n",
        "\n",
        "\n",
        "n = 40_000_000   \n",
        "\n",
        "\n",
        "# Two big tables with same join key distribution (forces big shuffle join)\n",
        "left = (spark.range(0, n)\n",
        "            .withColumn(\"k\", (F.col(\"id\") % 3_000_000).cast(\"int\"))\n",
        "            .withColumn(\"v1\", (F.rand(seed=21) * 1000).cast(\"double\"))\n",
        "            .repartition(400, \"k\"))  # big shuffle pressure\n",
        "\n",
        "right = (spark.range(0, n)\n",
        "             .withColumn(\"k\", (F.col(\"id\") % 3_000_000).cast(\"int\"))\n",
        "             .withColumn(\"v2\", (F.rand(seed=22) * 1000).cast(\"double\"))\n",
        "             .repartition(400, \"k\"))\n",
        "\n",
        "# Force materialization so the cost is real\n",
        "_ = left.count()\n",
        "_ = right.count()\n",
        "\n",
        "print(\"Holding executors to simulate long backfill...\")\n",
        "time.sleep(180)  # 3 minutes\n",
        "\n",
        "# Big shuffle join (NOT broadcast)\n",
        "j = left.join(right, on=\"k\", how=\"inner\")\n",
        "\n",
        "# Heavy aggregation to amplify shuffle\n",
        "heavy = (j.groupBy(\"k\")\n",
        "          .agg(F.sum(\"v1\").alias(\"sum_v1\"),\n",
        "               F.sum(\"v2\").alias(\"sum_v2\"),\n",
        "               F.count(\"*\").alias(\"cnt\"))\n",
        "          .repartition(200))\n",
        "\n",
        "rows = heavy.count()  # force execution\n",
        "\n",
        "duration = round(time.time() - start_ts, 2)\n",
        "result = {\n",
        "    \"runId\": runId, \"workloadName\": workloadName, \"tier\": tier, \"weight\": weight,\n",
        "    \"startTimeUtc\": start_utc, \"durationSec\": duration,\n",
        "    \"metric\": {\"resultRows\": rows, \"n\": n}\n",
        "}\n",
        "print(result)\n",
        "mssparkutils.notebook.exit(json.dumps(result))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        ""
      ]
    }
  ]
}